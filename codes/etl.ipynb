{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ade206b770d4e87aa77b8901d9b2f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, \\\n",
    "StringType as Str, IntegerType as Int, DateType as Date, LongType as Long, TimestampType as TS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d3a9640b284712bac0f00b8adf4fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def etl(input_data, input_key, output_data, output_key='analytics', start_datetime='1900-01-01 00:00:00', end_datetime='2200-12-31 12:59:59',\n",
    "        dq_checks=[], load_on_error=False):\n",
    "    \"\"\"\n",
    "    Function to join the trips records data and  weather data,\n",
    "    and write resulting analytics data to S3 in parquet format\n",
    "    \n",
    "    Argements:\n",
    "    input_data: source bucket of the trip record data \n",
    "    input_key: key of the trip record data\n",
    "    output_data: destination bucket for the analytics data\n",
    "    output_key: key for the analytics data\n",
    "    start_datetime: The expected start datetime for the trip records data. All records with earlier pickup_datetime\n",
    "    will be removed.\n",
    "    end_datetime:The expected end datetime for the trip records data. All records with later pickup_datetime\n",
    "    will be removed.\n",
    "    dq_checks: data quality checks on the analytics table. should be a list of dictionary with the following keys:\n",
    "    {'name': name of the test,\n",
    "    'sql_query': test sql query (the name of the view is 'sql_view'),\n",
    "    'expected_result': expected output of the sql_query\n",
    "    }\n",
    "    load_on_error: whether to load the analytics table if there is error \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Processing data...')\n",
    "\n",
    "    \n",
    "    ######################\n",
    "    #process weather data\n",
    "    ######################\n",
    "    \n",
    "    #load data\n",
    "    weather = spark.read.json(input_data + 'weather/weather.json')\n",
    "\n",
    "    #create datetime column in Eastern Standard Time (EST) to match trip record data \n",
    "    weather = weather.withColumn('datetime', F.when(F.col('timezone') == -18000, F.from_unixtime(F.col('dt')) - F.expr('INTERVAL 5 HOURS')) \\\n",
    "                                 .otherwise(F.from_unixtime(F.col('dt')) - F.expr('INTERVAL 4 HOURS')))\n",
    "\n",
    "    #remove duplicate due to daylight saving (e.g. record for 2019-11-03 01:00:00 EST appears twice)\n",
    "    weather = weather.drop_duplicates(['datetime'])\n",
    "\n",
    "    #create year, month, day and hour columns\n",
    "    weather = weather.withColumn('hour',F.hour('datetime')) \\\n",
    "        .withColumn('day',F.dayofmonth('datetime')) \\\n",
    "        .withColumn('month',F.month('datetime')) \\\n",
    "        .withColumn('year',F.year('datetime'))\n",
    "\n",
    "    #select relevant weather columns\n",
    "    weather = weather.select(['year',\n",
    "                              'month',\n",
    "                              'day',\n",
    "                              'hour',\n",
    "                               F.col('clouds.all').alias('cloudiness_pct'),\n",
    "                               F.col('main.temp').alias('temperature'),\n",
    "                              F.col('main.feels_like').alias('temp_feel_like'),\n",
    "                              F.col('main.temp_max').alias('temperature_max'),\n",
    "                              F.col('main.temp_min').alias('temperature_min'),\n",
    "                              F.col('main.humidity').alias('humidity'),\n",
    "                              F.col('main.pressure').alias('pressure'),\n",
    "                              F.col('rain.1h').alias('rain_last_1h'),\n",
    "                              F.col('rain.3h').alias('rain_last_3h'),\n",
    "                              F.col('snow.1h').alias('snow_last_1h'),\n",
    "                              F.col('snow.3h').alias('snow_last_3h'),\n",
    "                              F.col('weather')[0].main.alias('weather_type'),\n",
    "                              F.col('weather')[0].description.alias('weather_description'),\n",
    "                              F.col('wind').speed.alias('wind_speed'),\n",
    "                              F.col('wind').deg.alias('wind_degree')\n",
    "    ])\n",
    "\n",
    "\n",
    "    ######################\n",
    "    #Process trip record data\n",
    "    ######################\n",
    "\n",
    "    #load trips record data\n",
    "    trips = spark.read.csv(input_data + input_key, header=True, inferSchema=True)\n",
    "\n",
    "    #remove all data with NULL VendorID\n",
    "    trips = trips.filter(F.col('VendorID').isNotNull())\n",
    "\n",
    "    #remove all data out of date range\n",
    "    trips = trips.filter((F.col('tpep_pickup_datetime') >= start_datetime) & (F.col('tpep_pickup_datetime') <= end_datetime))\n",
    "\n",
    "    #load other dimensios table\n",
    "    vendors = spark.read.json(input_data + \"codes/vendor.json\")\n",
    "    payments = spark.read.json(input_data + \"codes/payment.json\")\n",
    "    rates = spark.read.json(input_data + \"codes/ratecode.json\")\n",
    "\n",
    "    #need separate table for pickup and dropoff to avoid cross join issue \n",
    "    taxizones_pickup = spark.read.csv(input_data + \"codes/taxizone.csv\", header=True, inferSchema=True)\n",
    "    taxizones_dropoff = spark.read.csv(input_data + \"codes/taxizone.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    #join reference tables\n",
    "    trips = trips.join(vendors, (trips.VendorID == vendors.id), 'left')\n",
    "    trips = trips.join(payments, (trips.payment_type == payments.id), 'left')\n",
    "    trips = trips.join(rates, (trips.RatecodeID == rates.id),'left')\n",
    "\n",
    "    #join taxizones for pickup location\n",
    "    trips = trips.join(taxizones_pickup.select(['LocationID',F.col('Borough').alias('pickup_borough'),\n",
    "                                         F.col('Zone').alias('pickup_zone')]),\n",
    "                       trips.PULocationID == taxizones_pickup.LocationID, 'left')\n",
    "\n",
    "    #join taxizones for dropoff location\n",
    "    trips = trips.join(taxizones_dropoff.select(['LocationID',F.col('Borough').alias('dropoff_borough'),\n",
    "                                                 F.col('Zone').alias('dropoff_zone')]),\n",
    "                       trips.DOLocationID == taxizones_dropoff.LocationID, 'left_outer')\n",
    "\n",
    "    #calcuation trip duration\n",
    "    trips = trips.withColumn('trip_duration', \n",
    "                             (F.col('tpep_dropoff_datetime').cast(Long()) - F.col('tpep_pickup_datetime').cast(Long()))/60)\n",
    "\n",
    "    trips = trips.select(['provider',\n",
    "                          F.col('tpep_pickup_datetime').alias('pickup_datetime'),\n",
    "                          F.col('tpep_dropoff_datetime').alias('dropoff_datetime'),\n",
    "                          'pickup_borough',\n",
    "                          'pickup_zone',\n",
    "                          'dropoff_borough',\n",
    "                          'dropoff_zone',\n",
    "                          'trip_duration',\n",
    "                          'trip_distance',\n",
    "                          'fare_amount',\n",
    "                          'extra',\n",
    "                          'mta_tax',\n",
    "                          'tip_amount',\n",
    "                          'tolls_amount',\n",
    "                          'improvement_surcharge',\n",
    "                          'congestion_surcharge',\n",
    "                          'total_amount',\n",
    "                          'payment',\n",
    "                          'rate',\n",
    "                          'store_and_fwd_flag'])\n",
    "\n",
    "    #############################\n",
    "    #Create time dimension table\n",
    "    ############################\n",
    "\n",
    "    time = trips.select(F.col('pickup_datetime')).distinct().withColumn('hour',F.hour('pickup_datetime')) \\\n",
    "        .withColumn('day',F.dayofmonth('pickup_datetime')) \\\n",
    "        .withColumn('week',F.weekofyear('pickup_datetime')) \\\n",
    "        .withColumn('month',F.month('pickup_datetime')) \\\n",
    "        .withColumn('year',F.year('pickup_datetime')) \\\n",
    "        .withColumn('weekday',F.dayofweek('pickup_datetime'))\n",
    "\n",
    "    #############################\n",
    "    #Join trip and weather tables\n",
    "    ############################\n",
    "\n",
    "    #join through the time table\n",
    "    analytics = trips.join(time, ['pickup_datetime'],'left').join(weather,['year','month','day','hour'], 'left')\n",
    "\n",
    "    print('Data processing completed...')\n",
    "\n",
    "    #############################\n",
    "    #Perform check\n",
    "    ############################\n",
    "\n",
    "    analytics.createOrReplaceTempView(\"sql_view\")\n",
    "\n",
    "    error_count = 0\n",
    "    num_of_test = len(dq_checks)\n",
    "\n",
    "    for test_num, dq_check in enumerate(dq_checks,1):\n",
    "        \n",
    "        \n",
    "        name = dq_check['name']\n",
    "        sql_query = dq_check['sql_query']\n",
    "        expected_result = dq_check['expected_result']\n",
    "\n",
    "        print(f'running data quality test {test_num} of {num_of_test}...')\n",
    "        print(f'test name: {name}')\n",
    "\n",
    "        result = spark.sql(sql_query).collect()[0][0]\n",
    "\n",
    "        if result == expected_result:\n",
    "            print(f'result: test passed')\n",
    "        else:\n",
    "            print(f'result: test failed. Result is {result}. Expected result is {expected_result}.')\n",
    "            error_count += 1\n",
    "\n",
    "    #############################\n",
    "    #load analytics table to output s3\n",
    "    ############################\n",
    "\n",
    "    if (error_count > 0) & (load_on_error == False):\n",
    "        print('Data tables not loaded due to failed data quality cuecks')\n",
    "    else:\n",
    "        print('loading data to s3...')\n",
    "        analytics.write.partitionBy('year','month').parquet(os.path.join(output_data, output_key), 'overwrite')\n",
    "        print('loading completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00210aa64b749738fc4cd01b2754bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Data processing completed...\n",
      "running data quality test 1 of 2...\n",
      "test name: check number of missing provider\n",
      "result: test passed\n",
      "running data quality test 2 of 2...\n",
      "test name: check number of missing temperature value\n",
      "result: test passed\n",
      "loading data to s3...\n",
      "loading completed."
     ]
    }
   ],
   "source": [
    "# Run ETL Function\n",
    "input_data = \"s3://nyc-yellow-cab-project/\"\n",
    "input_key = f\"tripdata/2019/*.csv\"\n",
    "output_data = \"s3://nyc-yellow-cab-project/\"\n",
    "output_key = 'analytics'\n",
    "start_datetime = '2019-01-01 00:00:00'\n",
    "end_datetime = '2019-12-31 23:59:59'\n",
    "dq_checks = [{\"name\":\"check number of missing provider\",\n",
    "\"sql_query\": \"\"\"SELECT COUNT(*) FROM sql_view WHERE PROVIDER IS NULL\"\"\",\n",
    "\"expected_result\": 0\n",
    "},\n",
    "{\"name\": \"check number of missing temperature value\",\n",
    "\"sql_query\": \"\"\"SELECT COUNT(*) FROM sql_view WHERE TEMPERATURE IS NULL\"\"\",\n",
    "\"expected_result\": 0\n",
    "}\n",
    "]\n",
    "\n",
    "load_on_error = False\n",
    "\n",
    "etl(input_data, input_key,output_data, output_key,start_datetime,end_datetime,dq_checks,load_on_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
